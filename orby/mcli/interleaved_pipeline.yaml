name: verl-interleaved

image: whatcanyousee/verl:ngc-cu124-vllm0.8.3-sglang0.4.5-mcore0.12.0-te2.2

integrations:
  - integration_type: git_repo
    git_repo: orby-ai-engineering/verl
    git_branch: main
    pip_install: .
    ssh_clone: true

compute:
  gpus: 64 # Number of GPUs to use
  cluster: r8z13p2
  gpu_type: h100_80gb

command: |
  # Set these variables before running the script.
  export EXPERIMENT_NAME="SET EXPERIMENT NAME"
  export PROJECT_NAME=verl_interleaved
  export MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct
  # Set this if you want to skip the initial SFT step and start from a checkpoint
  export BASE_SFT_CHECKPOINT=

  # Set the number of steps to interleave. Make sure the data directory contains the data for all steps.
  export INTERLEAVED_STEP_NUM=1

  # Set data paths
  export LOCAL_DATA_DIR=$HOME/data
  export LOCAL_MODEL_DIR=$HOME/model
  export LOCAL_EVAL_DIR=$HOME/eval
  export S3_CHECKPOINT_DIR=s3://orby-llm/interleaved/checkpoints/$EXPERIMENT_NAME

  # This should contain the training data for all steps. For example,
  # if INTERLEAVED_STEP_NUM=2, then it should contain:
  # 0/train.parquet, 1/train.parquet
  # We use shared validation files for all steps below.
  # Example: https://us-east-1.console.aws.amazon.com/s3/buckets/orby-llm?region=us-east-1&bucketType=general&prefix=interleaved/data/subtask/
  export INTERLEAVED_DATA_DIR=s3://orby-llm/interleaved/data/subtask

  # Change these to the initial SFT data files. Currently, it's also downloaded from INTERLEAVED_DATA_DIR
  # which contains sft_train.parquet.
  export INITIAL_SFT_TRAIN_FILES=$LOCAL_DATA_DIR/sft_train.parquet

  # Change this to the shared validation files for all steps.
  export SHARED_VAL_FILES=$LOCAL_DATA_DIR/test.parquet

  # Set batch sizes based on the number of GPUs. Note max sft micro batch size per GPU is 4.
  # Rollout batch size 1024 is tested, larger rollout batch size may be feasible, but may cause OOM
  # on the main node as it needs to load the entire rollout batch into memory.
  export NUM_NODES=8
  export SFT_MICRO_BATCH_SIZE_PER_GPU=2
  export SFT_TRAIN_BATCH_SIZE=128
  export GRPO_MICRO_BATCH_SIZE_PER_GPU=1
  export GRPO_TRAIN_BATCH_SIZE=64
  export ROLLOUT_BATCH_SIZE=1024

  # Set SFT parameters
  export ATTENTION_DROPOUT=0.0
  export SFT_LR=1e-5

  # Set GRPO parameters
  export REWARD_FN=training_reward_func
  export REWARD_FILE=orby/reward/subtask.py
  export COORDINATES_METRIC="gaussian"
  export COORDINATES_GAUSSIAN_SIGMA=2
  export COORDINATES_PIXEL_SQUARE_SIZE=10
  export GRPO_LR=1e-6
  export GENERATED_DATA_RESPONSE_KEY=predictions

  # Set eval parameters
  export EVAL_REWARD_FN=eval_reward_func
  export EVAL_OUTPUT_FILE=$LOCAL_DATA_DIR/tmp_eval_output.parquet
  export LOCAL_EVAL_RESULT_FILE=$LOCAL_EVAL_DIR/results.parquet
  export S3_EVAL_RESULT_DIR=s3://orby-llm/interleaved/eval/$EXPERIMENT_NAME
  export S3_EVAL_RESULT_FILE=$S3_EVAL_RESULT_DIR/results.parquet

  # Set difficulty filter parameters
  export MEDIUM_DIFFICULTY_FILTER_UPPER_BOUND=0.9
  export MEDIUM_DIFFICULTY_FILTER_LOWER_BOUND=0.51
  export HARD_DIFFICULTY_FILTER_UPPER_BOUND=0.5
  export HARD_DIFFICULTY_FILTER_LOWER_BOUND=0.09
  export REWARD_SCORE_COLUMN="reward_score"

  # Set rollout parameters
  export S3_ROLLOUT_OUTPUT_DIR=s3://orby-llm/interleaved/rollout/$EXPERIMENT_NAME
  export N_SAMPLES=4
  export TEMPERATURE=0.7

  # Init environment
  cd /workspace/verl
  sed -i 's|mirrors.tuna.tsinghua.edu.cn|us.archive.ubuntu.com|g' /etc/apt/sources.list
  apt update
  apt install iproute2 -y
  apt install -y dnsutils
  apt install -y awscli
  pip install 'urllib3<2'
  pip install s3fs
  pip install sgl-kernel
  pip install boto3
  pip install parquet-tools
  pip install transformers==4.53.0

  # Set environment variables
  export HYDRA_FULL_ERROR=1
  INTERFACE=$(ip route | grep default | awk '{print $5}' | head -1)
  echo "Using interface: $INTERFACE"
  export GLOO_SOCKET_IFNAME=$INTERFACE
  export HF_HUB_ENABLE_HF_TRANSFER=1
  export CUDA_LAUNCH_BLOCKING=1

  # Install verl lib: https://verl.readthedocs.io/en/latest/start/install.html
  pip3 install -e .[vllm]

  # Download merged datasets
  aws s3 cp --no-progress --recursive $INTERLEAVED_DATA_DIR $LOCAL_DATA_DIR

  # Run interleaved pipeline
  echo "TOP LEVEL - Now starting the interleaved training pipeline at orby/scripts/interleaved_pipeline.sh"
  bash orby/scripts/interleaved_pipeline.sh
