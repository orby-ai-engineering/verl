name: verl-finetune-sft-subtask

image: whatcanyousee/verl:ngc-cu124-vllm0.8.3-sglang0.4.5-mcore0.12.0-te2.2

integrations:
  - integration_type: git_repo
    git_repo: orby-ai-engineering/verl
    git_branch: main
    pip_install: .
    ssh_clone: true

compute:
  gpus: 32 # Number of GPUs to use
  cluster: r8z13p2
  gpu_type: h100_80gb

command: |
  # Set these variables before running the script.
  export EXPERIMENT_NAME="SET THE EXPERIMENT NAME HERE"
  export NUM_NODES=4
  export TRAIN_BATCH_SIZE=256
  export MODEL_NAME=Qwen/Qwen2.5-VL-7B-Instruct
  export PROJECT_NAME=verl_sft_subtask
  export S3_CHECKPOINT_DIR=s3://orby-osu-va/verl-checkpoints/subtask_sft/$EXPERIMENT_NAME

  # Set training environment variables
  export TRAIN_FILES=$HOME/data/subtask_direct_distill/mix/train/combined_with_response.parquet
  export VAL_FILES=$HOME/data/subtask_direct_distill/mix/test/combined_with_response.parquet

  # Init environment
  cd /workspace/verl
  sed -i 's|mirrors.tuna.tsinghua.edu.cn|us.archive.ubuntu.com|g' /etc/apt/sources.list
  apt update
  apt install iproute2 -y
  apt install -y dnsutils
  apt install -y emacs
  apt install -y awscli
  pip install 'urllib3<2'
  pip install s3fs
  pip install parquet-tools
  pip install sgl-kernel
  pip install boto3

  # Set environment variables
  export HYDRA_FULL_ERROR=1
  INTERFACE=$(ip route | grep default | awk '{print $5}' | head -1)
  echo "Using interface: $INTERFACE"
  export GLOO_SOCKET_IFNAME=$INTERFACE
  export HF_HUB_ENABLE_HF_TRANSFER=1

  # Downloads the model
  python3 -c "import transformers; transformers.pipeline(model='$MODEL_NAME', device='cpu')"

  # Install verl lib: https://verl.readthedocs.io/en/latest/start/install.html
  pip3 install -e .[vllm]

  # Set PYTHONPATH for orby.trainer module
  export PYTHONPATH="/workspace/verl:$PYTHONPATH"

  # Download merged datasets
  mkdir -p ~/data/subtask_direct_distill/mix/train/
  mkdir -p ~/data/subtask_direct_distill/mix/test/
  aws s3 cp s3://orby-osu-va/subtask/verl/experiment_2/test/executor_reward_model_combined_with_response/part-00000-tid-6116228279497623000-6660e492-d87d-4c87-903b-261c86c79b92-531-1-c000.snappy.parquet $VAL_FILES
  aws s3 cp s3://orby-osu-va/subtask/verl/experiment_2/train/executor_reward_model_combined_with_response/part-00000-tid-3712964276653840281-af2210b2-e910-4427-aa16-9f2a2cfdae0a-844-1-c000.snappy.parquet $TRAIN_FILES

  # Run torchrun on each node
  torchrun \
    --nproc_per_node=8 \
    --nnodes=$NUM_NODES \
    --node_rank=$NODE_RANK \
    --master_addr=$MASTER_ADDR \
    --master_port=${MASTER_PORT:-29500} \
    -m orby.trainer.fsdp_sft_trainer \
    data.train_batch_size=$TRAIN_BATCH_SIZE \
    data.micro_batch_size_per_gpu=8 \
    data.train_files=$TRAIN_FILES \
    data.val_files=$VAL_FILES \
    +data.max_prompt_length=7680 \
    +data.max_response_length=512 \
    +data.filter_overlong_prompts=False \
    data.truncation='error' \
    +data.shuffle=True \
    data.prompt_key=prompt \
    data.response_key=response \
    +data.image_key=images \
    +processor.use_fast=true \
    +processor.trust_remote_code=true \
    optim.lr=1e-6 \
    model.partial_pretrain=$MODEL_NAME \
    model.fsdp_config.cpu_offload=true \
    model.enable_gradient_checkpointing=true \
    +model.enable_activation_offload=true \
    model.fsdp_config.offload_params=true \
    +model.fsdp_config.param_offload=true \
    trainer.default_local_dir=$S3_CHECKPOINT_DIR \
    trainer.total_training_steps=null \
    trainer.project_name=$PROJECT_NAME \
    trainer.experiment_name=$EXPERIMENT_NAME \
    trainer.logger=[console,wandb] \
    trainer.default_hdfs_dir=null \
    +trainer.val_interval=100 \
    +trainer.save_interval=100 \
    trainer.total_epochs=1 \
    ulysses_sequence_parallel_size=1 \
    use_remove_padding=false \
    +model.fsdp_config.reshard_after_forward=true \
    +model.use_remove_padding=true \
    model.fsdp_config.wrap_policy.min_num_params=1000000 \
    +model.fsdp_config.optimizer_offload=true
