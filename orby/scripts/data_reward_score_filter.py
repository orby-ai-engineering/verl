#!/usr/bin/env python3
"""
Filter dataset based on reward scores generated by main_eval.

This script takes a parquet dataset with reward scores and filters out rows where any 
specified score exceeds its corresponding threshold.

Usage:
    python data_reward_score_filter.py --input_path input.parquet --output_path filtered.parquet --thresholds '{"reward_accuracy": 0.8, "reward_completion": 0.9}'
"""

import argparse
import json
import os
import pandas as pd
from typing import Dict


def filter_by_thresholds(df: pd.DataFrame, thresholds: Dict[str, float]) -> pd.DataFrame:
    """
    Filter dataframe based on column-specific thresholds.
    Remove rows where ANY of the specified columns exceeds its threshold.
    
    Args:
        df: Input dataframe
        thresholds: Dictionary mapping column names to threshold values
    
    Returns:
        Filtered dataframe
    """
    if not thresholds:
        print("Warning: No thresholds specified. Returning original dataset.")
        return df
    
    # Check that all specified columns exist
    missing_columns = [col for col in thresholds.keys() if col not in df.columns]
    if missing_columns:
        raise ValueError(f"Columns not found in dataset: {missing_columns}")
    
    print(f"Filtering with thresholds: {thresholds}")
    
    original_count = len(df)
    
    # Start with all rows included
    mask = pd.Series([True] * len(df), index=df.index)
    
    # For each column-threshold pair, exclude rows that exceed the threshold
    for col, threshold in thresholds.items():
        # Exclude rows where the score is above threshold (keeping NaN values)
        col_mask = (df[col].isna()) | (df[col] <= threshold)
        mask = mask & col_mask
        
        # Show progress for each column
        excluded_by_this_col = (~col_mask).sum()
        print(f"  {col} > {threshold}: excluding {excluded_by_this_col} rows")
    
    filtered_df = df[mask].copy()
    filtered_count = len(filtered_df)
    removed_count = original_count - filtered_count
    
    print(f"\nOriginal dataset size: {original_count}")
    print(f"Filtered dataset size: {filtered_count}")
    print(f"Removed {removed_count} rows ({removed_count/original_count*100:.1f}%)")
    
    return filtered_df


def load_thresholds(thresholds_str: str) -> Dict[str, float]:
    """Load thresholds from a JSON string."""
    try:
        return json.loads(thresholds_str)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in thresholds string: {e}")


def main():
    parser = argparse.ArgumentParser(
        description="Filter dataset based on column-specific reward score thresholds",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Filter using JSON string with different thresholds per column
  python data_reward_score_filter.py --input_path dataset.parquet --output_path filtered.parquet --thresholds '{"reward_accuracy": 0.8, "reward_completion": 0.9}'
        """
    )
    
    parser.add_argument(
        "--input_path", 
        type=str, 
        required=True,
        help="Path to input parquet file"
    )
    
    parser.add_argument(
        "--output_path", 
        type=str,
        required=True,
        help="Path to output filtered parquet file"
    )
    
    parser.add_argument(
        "--thresholds", 
        type=str,
        required=True,
        help="JSON string mapping column names to threshold values, e.g., '{\"reward_accuracy\": 0.8, \"reward_completion\": 0.9}'"
    )
    
    args = parser.parse_args()
    
    if not os.path.exists(args.input_path):
        raise FileNotFoundError(f"Input file not found: {args.input_path}")
    
    # Load dataset
    print(f"Loading dataset from: {args.input_path}")
    df = pd.read_parquet(args.input_path)
    print(f"Dataset shape: {df.shape}")
    
    # Load thresholds
    thresholds = load_thresholds(args.thresholds)
    
    # Filter dataset
    filtered_df = filter_by_thresholds(df, thresholds)
    
    # Save filtered dataset
    output_dir = os.path.dirname(args.output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
    
    filtered_df.to_parquet(args.output_path, index=False)
    print(f"\nFiltered dataset saved to: {args.output_path}")


if __name__ == "__main__":
    main()
